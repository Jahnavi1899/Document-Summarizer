arXiv:2310.10449v2  [cs.CL]  17 Oct 2023Text Summarization Using Large Language Models:
A Comparative Study of MPT-7b-instruct,
Falcon-7b-instruct, and OpenAI Chat-GPT Models
Lochan Basyal
Mentee, KaggleX BIPOC Mentorship Program Cohort 3
bashyallochan@gmail.comMihir Sanghvi
Mentor, KaggleX BIPOC Mentorship Program Cohort 3
mihir2891@gmail.com
Abstract —Text summarization is a critical Natural Language
Processing (NLP) task with applications ranging from infor ma-
tion retrieval to content generation. Leveraging Large Lan guage
Models (LLMs) has shown remarkable promise in enhancing
summarization techniques. This paper embarks on an explora tion
of text summarization with a diverse set of LLMs, including
MPT-7b-instruct, falcon-7b-instruct, and OpenAI ChatGPT text-
davinci-003 models. The experiment was performed with diff er-
ent hyperparameters and evaluated the generated summaries
using widely accepted metrics such as the Bilingual Evalua-
tion Understudy (BLEU) Score, Recall-Oriented Understudy for
Gisting Evaluation (ROUGE) Score, and Bidirectional Encod er
Representations from Transformers (BERT) Score. Accordin g to
the experiment, text-davinci-003 outperformed the others . This
investigation involved two distinct datasets: CNN Daily Ma il and
XSum. Its primary objective was to provide a comprehensive
understanding of the performance of Large Language Models
(LLMs) when applied to different datasets. The assessment
of these models’ effectiveness contributes valuable insig hts to
researchers and practitioners within the NLP domain. This
work serves as a resource for those interested in harnessing the
potential of LLMs for text summarization and lays the founda tion
for the development of advanced Generative AI applications
aimed at addressing a wide spectrum of business challenges.
Index Terms —Text Summarization, MPT-7b-instruct, Falcon-
7b-instruct, OpenAI ChatGPT
I. I NTRODUCTION
In the era of Big Data, the abundance of textual information
has underscored the importance of efﬁcient text summariza-
tion techniques. Text summarization, the task of distillin g
long documents or articles into concise, coherent summarie s
while preserving the core meaning and essential informatio n,
holds immense value across various domains. From aiding
in information retrieval to content generation, summariza tion
has emerged as a pivotal component of Natural Language
Processing (NLP) applications.
Recent advancements in NLP have been characterized
by the rise of Large Language Models (LLMs), OpenAI
ChatGPT[1], MPT-7b-instruct[2], [3], ﬂan-t5-xl[4], falc on-7b-
instruct[5], [6], and others, which have demonstrated rema rk-
able capabilities in understanding and generating human-l ike
text. These LLMs have opened new avenues for text summa-
rization by providing powerful generative capabilities an d the
ability to adapt to diverse tasks through ﬁne-tuning.The ’text-davinci-003 (Legacy)’[1] model represents a re-
markable leap in the ﬁeld of Natural Language Processing
(NLP). It exhibits an unparalleled ability to handle a wide
range of language tasks with exceptional precision and qual ity.
Notably, it surpasses its predecessors, including the Curi e,
babbage, and Ada models, in terms of generating text of highe r
quality, offering longer outputs, and consistently follow ing
provided instructions. This legacy model has a token capaci ty
of 4,097, enabling it to handle extensive text generation wi th
ease. Moreover, ’text-davinci-003 (Legacy)’ introduces i nno-
vative features, such as the capability to insert text seaml essly
into generated content, thereby expanding its utility for d iverse
text manipulation tasks.
The ’MPT-7B-Instruct’ model, as cited in[2], [3], is speciﬁ -
cally designed for short-form instruction-following task s, mak-
ing it an ideal choice for a wide range of instruction-based
applications. It is created through the ﬁne-tuning process of
a base model, MPT-7B, using a dataset sourced from the
Databricks Dolly-15k and the Anthropic Helpful and Harmles s
(HH-RLHF) datasets. This tailored approach results in a
model that excels at understanding and following instructi ons
with precision and accuracy. The model follows a modiﬁed
decoder-only transformer architecture, optimized for sup erior
performance in instruction-following tasks.
Falcon-7B-Instruct’, as cited in[5], [6], represents a
formidable 7 billion-parameter causal decoder-only model
meticulously crafted by the Technology Innovation Institu te
(TII). This model is built upon the robust foundation of Falc on-
7B and undergoes a ﬁne-tuning process using a composite
dataset sourced from both chat and instruct domains. ’Falco n-
7B-Instruct’ is generously made available under the Apache
2.0 license.
The focus of this paper is to delve into the world of
text summarization with LLMs, offering a comprehensive
exploration of their potential and limitations. Speciﬁcal ly, we
investigate various LLMs, experiment with different hyper pa-
rameters, and evaluate the quality of summaries generated b y
these models. To ensure a robust evaluation, we employ well-
established metrics such as BLEU Score, Rouge Score, and
Bert Score.
This paper serves as a vital resource for those seeking to
harness the power of LLMs for NLP applications and lays thegroundwork for the development of advanced Generative AI
solutions to address a wide range of business challenges. In the
following sections, the paper provides detailed explanati ons of
the text summarization methods discussed in Section II, sup er-
vised and unsupervised summarization in Section III, datas ets
and evaluation metrics presented in Section IV , inference w ith
different LLMs in Section V , and offers a roadmap for future
enhancements, concluding with Section VI. Lastly, the auth or
acknowledges the support received during the research and
experiments.
II. T EXT SUMMARIZATION METHODS
Text summarization is a fundamental task in Natural Lan-
guage Processing (NLP) that aims to condense large volumes
of text into shorter, coherent representations while prese rving
the essential information. There are primarily two approac hes
to text summarization: abstractive and extractive summari za-
tion.
A. Abstractive Text Summarization
Abstractive summarization involves generating a concise
summary that may contain words, phrases, or sentences not
present in the source text. This approach relies on understa nd-
ing the context and generating human-like language to conve y
the central ideas. Abstractive summarization methods ofte n use
advanced language models, such as Large Language Models
(LLMs), to rewrite and rephrase content in a more concise
form.
B. Extractive Text Summarization
Extractive summarization, on the other hand, aims to select
and extract the most important sentences or phrases directl y
from the source text to form the summary. It does not involve
rephrasing or generating new sentences. Extractive summar iza-
tion methods use various techniques, such as sentence scori ng
and ranking, to identify and extract the most salient conten t.
III. S UPERVISED AND UNSUPERVISED SUMMARIZATION
Text summarization techniques can be broadly categorized
into two main approaches based on dataset labeling: super-
vised and unsupervised summarization. Each approach has it s
methodologies and advantages, serving different use cases and
data availability scenarios.
A. Supervised Summarization
Supervised summarization is a method that relies on labeled
training data, where human annotators provide summaries
for a given set of source texts. Machine learning models
are then trained on this data to learn the mapping between
source texts and their corresponding summaries. This appro ach
is particularly effective when high-quality, domain-spec iﬁc
summaries are available for training.B. Unsupervised Summarization
Unsupervised summarization, on the other hand, does not
require labeled training data. Instead, it seeks to extract
the most relevant information from the source text using
algorithms that consider factors like sentence importance ,
coherence, and redundancy. Unsupervised methods are often
employed when labeled summarization datasets are scarce or
costly to obtain.
IV. D ATASETS AND EVALUATION METRICS
In our study, we conducted experiments and evaluations
on two distinct datasets, CNN/Daily Mail 3.0.0[7] and the
Extreme Summarization (XSum)[8] to assess the performance
of various Large Language Models (LLMs) in the context of
text summarization. These datasets serve as the foundation for
our evaluation and comparison of LLM-generated summaries.
A. Datasets
•CNN/Daily Mail 3.0.0 Dataset: The CNN/Daily Mail
3.0.0 Dataset, a valuable resource in the realm of nat-
ural language processing, comprises more than 300,000
unique news articles authored by journalists from CNN
and the Daily Mail. Originally designed to facilitate ma-
chine reading and comprehension, this English-language
dataset has since evolved to support both extractive and
abstractive summarization tasks. The dataset provides
three key data ﬁelds for each entry: ’id,’ which contains
the hexadecimal-formatted SHA1 hash of the URL from
which the story was retrieved; ’article,’ which contains th e
body of the news article itself; and ’highlights,’ featurin g
the article’s highlights as written by the original author.
•XSum Dataset: XSum dataset is a valuable resource
tailored for extreme summarization tasks. It consists of
news articles with three key features: the ’document,’
serving as the input news article, the ’summary’ providing
a one-sentence summary of the article, and the ’id,’ which
uniquely identiﬁes each article using the BBC ID.
The inclusion of these diverse datasets allows us to evalu-
ate the performance of LLMs across various content types,
ensuring that our study provides a holistic view of their
summarization capabilities.
B. Evaluation Metrics
To assess the quality and effectiveness of the generated
summaries, we employed a set of widely accepted evaluation
metrics:
•BLEU Score[9]: BLEU is a metric employed to assess
the quality of machine translations. It operates by measur-
ing the similarity between n-grams present in machine-
translated sentences and those in human-translated sen-
tences. It is generally noted that the BLEU score tends to
decrease with longer sentence lengths, although variation s
in this trend can occur depending on the translation model
in use.
•ROUGE Score[10], [12]: The ROUGE Score assesses
the overlap of n-grams (sequences of words) between thegenerated summary and reference summaries. It considers
metrics such as ROUGE-N (unigrams, bigrams, etc.) and
ROUGE-L (longest common subsequence) to evaluate
content overlap.
•BERT Score[11], [12]: The BERT Score utilizes contex-
tual embeddings from the BERT model to measure the
similarity between the generated summary and reference
summaries. It is designed to capture the nuances of lan-
guage and context, providing a robust evaluation metric.
By calculating these metrics for summaries generated with
different LLMs, we aim to provide a comprehensive assess-
ment of their performance, enabling researchers and practi -
tioners to make informed decisions when choosing an LLM
and ﬁne-tuning their summarization models for speciﬁc task s
and datasets.
V. I NFERENCE WITH DIFFERENT LLM S
In this section, the results of the experiments are presente d,
wherein a variety of Large Language Models (LLMs) were
utilized to generate summaries for two distinct datasets. T he
LLMs employed for these experiments include falcon-7b-
instruct, mpt-7b-instruct, and text-davinci-003. The pri mary
objective is to offer a comparative analysis of their perfor -
mance concerning text summarization.
A. Experiment Setup
For each LLM, experiments were conducted using a temper-
ature value of 0.1 and a maximum token length of 100. These
experiments involved summarizing 25 test samples of each
dataset. The process of generating the text summary entaile d
the utilization of LangChain and Hugging Face pipelines
for prompt engineering, ensuring precision and efﬁciency i n
the summarization process. This experiment was executed
by hosting custom Google Compute Engine Virtual Machine
(GCE VM) instances equipped with NVIDIA T4 Graphics
Processing Units (GPUs) sourced from the Google Cloud
Platform (GCP).
B. Results
The performance of different LLMs on two distinct datasets,
utilizing the speciﬁed temperature value, was displayed. M et-
rics were computed for each LLM, offering a comprehensive
perspective on their summarization capabilities, as avail able
on the GitHub repository cited in this paper[13].
These tables, as referenced in Table I and Table II, present a
comprehensive evaluation of various Large Language Models
(LLMs) for text summarization across two distinct datasets :
CNN/Daily Mail 3.0.0 and XSum. The performance of each
LLM is assessed using several key metrics, including BLEU,
ROUGE, and BERT.
The table highlights varying performance across LLMs and
datasets. Notably, the OpenAI model, text-davinci-003, co n-
sistently exhibits strong performance, achieving high BLE U,
ROUGE, and BERT Scores. This exceptional performance can
be attributed to davinci being the largest and most powerful
model, with 175 billion parameters and 45TB of text data.When comparing the two 7b parameter ﬁne-tuned models,
MPT-7b-instruct performed slightly better than Falcon-7b -
instruct. However, their overall performance was somewhat
similar. These ﬁndings underscore the signiﬁcance of model
architecture and size in text summarization tasks, as well a s
the potential of OpenAI’s model for achieving state-of-the -art
results in diverse NLP applications.
VI. C ONCLUSION AND FUTURE ENHANCEMENTS
This research embarked on a comprehensive exploration
of text summarization techniques using various Large Lan-
guage Models (LLMs), with the goal of shedding light on
their performance in different settings and scenarios. The
study encompassed the evaluation of LLMs such as mpt-7b-
instruct, falcon-7b-instruct, and text-davinci-003, as w ell as
their summarization capabilities across two diverse datas ets,
’CNN/Daily Mail 3.0.0’ and ’XSum.
The experiment results, as indicated by the model per-
formance table and human evaluation of the generated text
summaries, highlight the exceptional performance of OpenA I’s
model, text-davinci-003, in comparison to other models. Th ese
models consistently demonstrated a superior ability to pro duce
high-quality summaries across various datasets and temper a-
ture settings.
In the coming days, this work can be extended to lever-
age inferences from larger samples using higher-parameter
models, such as mosaicml/mpt-30b-instruct and tiiuae/fal con-
40b-instruct, potentially leading to even more robust and
accurate summarization results. Additionally, the human e val-
uation metrics and inferences can be generated from dataset s
with varying word counts and output token lengths. The
continual advancement of Large Language Models (LLMs)
with increasing model size and capabilities offers an excit ing
opportunity to explore how these models can further enhance
the quality of text summarization, translation, and conten t
generation. Moreover, the ﬁne-tuning of LLMs on speciﬁc
domains and datasets could unlock the potential for domain-
speciﬁc summarization models with exceptional performanc e.
In conclusion, this research contributes valuable insight s
into the ﬁeld of text summarization with LLMs and offers a
glimpse into future research directions. As the NLP landsca pe
continues to evolve, leveraging the capabilities of LLMs, e s-
pecially those offered by OpenAI, holds great promise for th e
development of advanced Generative AI applications across
diverse business domains.
ACKNOWLEDGMENT
The author would like to express heartfelt gratitude to Mihi r
Sanghvi, Mentor of the KaggleX BIPOC Mentorship Program
Cohort 3. Mihir’s invaluable guidance, mentorship, and in-
sights have signiﬁcantly contributed to the success of this
research. Additionally, appreciation is extended to Kaggl e for
providing the opportunity to participate in the KaggleX BIP OC
Mentorship Program, which facilitated the collaboration a nd
learning experiences that enriched this work.LLM Model Dataset Avg. Word Count ROUGE-1 ROUGE-2 ROUGE-L BE RT Score (P/R/F1)
falcon-7b-instruct CNN (n=25) 784.24 0.226 0.053 0.197 0.8 18 / 0.860 / 0.838
falcon-7b-instruct XSum (n=25) 410.44 0.139 0.014 0.113 0. 787 / 0.863 / 0.823
mpt-7b-instruct CNN (n=25) 784.24 0.236 0.060 0.213 0.839 / 0.864 / 0.851
mpt-7b-instruct XSum (n=25) 410.44 0.159 0.024 0.133 0.828 / 0.871 / 0.848
text-davinci-003 CNN (n=25) 784.24 0.272 0.096 0.255 0.854 / 0.883 / 0.868
text-davinci-003 XSum (n=25) 410.44 0.206 0.053 0.173 0.84 4 / 0.893 / 0.868
TABLE I: Performance Metrics of LLMs on ”CNN/Daily Mail 3.0. 0” and ”XSum” Datasets
LLM Model Dataset Avg. Word Count BLEU Score
falcon-7b-instruct CNN (n=25) 784.24 9.4726138403298e-2 32
falcon-7b-instruct XSum (n=25) 410.44 9.225829346520394 e-232
mpt-7b-instruct CNN (n=25) 784.24 9.35328936831654e-232
mpt-7b-instruct XSum (n=25) 410.44 9.542118736121376e-2 32
text-davinci-003 CNN (n=25) 784.24 0.4896200481408649
text-davinci-003 XSum (n=25) 410.44 0.48979461356547943
TABLE II: Performance Metrics, BLEU Score of LLMs on ”CNN/Da ily Mail 3.0.0” and ”XSum” Datasets
Furthermore, the support provided by Kaggle in the form of
the Kaggle-KaggleX Google Cloud Platform (GCP) Coupon is
acknowledged. This support enabled access to essential com -
puting resources on Google Cloud, which was instrumental
in conducting the experiments for this research. Gratitude is
expressed for the collective efforts of the Kaggle communit y,
which continues to foster a collaborative and innovative en vi-
ronment for data science and machine learning research.
REFERENCES
[1] OpenAI GPT-3.5, ”text-davinci-003,”. [Online]. Avail able:
https://platform.openai.com/docs/models/gpt-3-5. [Ac cessed: 2023-
10-14]
[2] MosaicML NLP Team, ”Introducing MPT-7B: A New Standard f or
Open-Source, Commercially Usable LLMs,” 2023. [Online]. A vailable:
www.mosaicml.com/blog/mpt-7b. [Accessed: 2023-10-14]
[3] ”MPT-7B-Instruct, Hugging Face Models.” [Online]. Ava ilable:
https://huggingface.co/mosaicml/mpt-7b-instruct. [Ac cessed: 2023-10-
14]
[4] Chung, Hyung Won, Le Hou, Shayne Longpre, Barret Zoph, Yi
Tay, William Fedus, Eric Li, Xuezhi Wang, Mostafa Dehghani, Sid-
dhartha Brahma, Albert Webson, Shixiang Shane Gu, Zhuyun Da i,
Mirac Suzgun, Xinyun Chen, Aakanksha Chowdhery, Sharan Nar ang,
Gaurav Mishra, Adams Yu, Vincent Zhao, Yanping Huang, Andre w
Dai, Hongkun Yu, Slav Petrov, Ed H. Chi, Jeff Dean, Jacob Devl in,
Adam Roberts, Denny Zhou, Quoc V . Le, and Jason Wei. ”Scaling
Instruction-Finetuned Language Models,” 2022. [Online]. Available:
https://arxiv.org/abs/2210.11416. [Accessed: 2023-10- 14]
[5] ”Falcon-7B-Instruct, Hugging Face Models.” 2023. [Onl ine]. Available:
https://huggingface.co/tiiuae/falcon-7b-instruct. [A ccessed: 2023-10-14]
[6] Almazrouei, Ebtesam and Alobeidli, Hamza and Alshamsi, Abdulaziz
and Cappelli, Alessandro and Cojocaru, Ruxandra and Debbah , Mer-
ouane and Gofﬁnet, Etienne and Heslow, Daniel and Launay, Ju lien and
Malartic, Quentin and Noune, Badreddine and Pannier, Bapti ste and
Penedo, Guilherme. ”Falcon-40B: an open large language mod el with
state-of-the-art performance.”
[7] ”CNN/DailyMail Dataset, Hugging Face Datasets.” [Onli ne]. Available:
https://huggingface.co/datasets/cnn dailymail. [Accessed: 2023-10-14]
[8] ”XSum Dataset, Hugging Face Datasets.” [Online]. Avail able:
https://huggingface.co/datasets/xsum [Accessed: 2023- 10-14]
[9] ”Metric Card for BLEU, Hugging Face Metrics.” [Online]. Available:
https://huggingface.co/spaces/evaluate-metric/bleu. [Accessed: 2023-10-
14][10] ”Metric Card for ROUGE, Hugging Face Metrics.” [Online ]. Available:
https://huggingface.co/spaces/evaluate-metric/rouge . [Accessed: 2023-
10-14].
[11] ”Metric Card for BERT Score, Hugging Face Metrics.” [On line].
Available: https://huggingface.co/spaces/evaluate-me tric/bertscore. [Ac-
cessed: 2023-10-14].
[12] Yan, Ziyou, ”Evaluation & Hallucination Detection for Abstractive Sum-
maries” [Online]. Available: https://eugeneyan.com/wri ting/abstractive/.
[Accessed: 2023-10-14]
[13] L. Basyal, ”LLMs-Text-Summarization,” GitHub. [Onli ne]. Avail-
able: https://github.com/lbasyal/LLMs-Text-Summariza tion. [Accessed:
2023-10-14]